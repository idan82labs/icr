"""
True RLM (Retrieval Language Model) Implementation.

Implements research-grade RLM based on:
- arXiv:2512.24601 "Retrieval-Augmented Language Models" (Chen et al., 2024)
- Context externalization: LLM writes code/queries to explore data
- Sub-LLM calls: Recursive retrieval with focused sub-agents
- Self-reflection: Quality evaluation and correction

Key differences from basic query expansion:
1. LLM explicitly "programs" retrieval operations
2. Operations execute in parallel where possible
3. Results are evaluated and may trigger refinement
4. Graph-aware exploration follows dependencies
"""

from __future__ import annotations

import asyncio
import json
import os
from dataclasses import dataclass, field
from enum import Enum
from typing import TYPE_CHECKING, Any

import structlog

if TYPE_CHECKING:
    from icd.config import Config
    from icd.graph.builder import CodeGraphBuilder
    from icd.retrieval.crag import CRAGRetriever
    from icd.retrieval.hybrid import Chunk, HybridRetriever, RetrievalResult

logger = structlog.get_logger(__name__)


class OperationType(str, Enum):
    """Types of retrieval operations in an RLM program."""

    SEMANTIC_SEARCH = "semantic_search"  # Vector similarity search
    SYMBOL_LOOKUP = "symbol_lookup"  # Find specific symbol
    GRAPH_TRAVERSE = "graph_traverse"  # Follow code graph edges
    FILE_GREP = "file_grep"  # Pattern search in files
    AGGREGATE = "aggregate"  # Combine results
    FILTER = "filter"  # Filter results
    EXPAND_CONTEXT = "expand_context"  # Get surrounding code


@dataclass
class RetrievalOperation:
    """A single retrieval operation in the RLM program."""

    op_id: str
    op_type: OperationType
    query: str
    parameters: dict[str, Any] = field(default_factory=dict)
    depends_on: list[str] = field(default_factory=list)  # IDs of prerequisite ops
    results: list["Chunk"] = field(default_factory=list)
    scores: list[float] = field(default_factory=list)
    completed: bool = False
    quality_score: float = 0.0


@dataclass
class RLMProgram:
    """A program of retrieval operations generated by the LLM."""

    original_query: str
    operations: list[RetrievalOperation]
    reasoning: str
    max_depth: int = 3
    current_depth: int = 0
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class RLMExecutionResult:
    """Result of executing an RLM program."""

    chunks: list["Chunk"]
    scores: list[float]
    operations_executed: int
    refinement_iterations: int
    final_entropy: float
    execution_trace: list[str]
    metadata: dict[str, Any] = field(default_factory=dict)


class TrueRLMOrchestrator:
    """
    True RLM orchestrator implementing research-grade retrieval.

    Key features:
    1. Context Externalization: LLM generates retrieval "programs"
    2. Parallel Execution: Independent ops run concurrently
    3. Dependency Resolution: Ops with prereqs wait for inputs
    4. Quality Evaluation: CRAG-style relevance checking
    5. Recursive Refinement: Low-quality results trigger re-planning
    6. Graph-Aware: Follows code dependencies
    """

    PROGRAM_GENERATION_PROMPT = '''You are a code retrieval expert. Given a user's question, generate a retrieval program.

A retrieval program is a list of operations that will find relevant code. Each operation has:
- id: Unique identifier (e.g., "op1", "op2")
- type: One of "semantic_search", "symbol_lookup", "graph_traverse", "expand_context"
- query: The search query or symbol name
- parameters: Additional parameters (optional)
- depends_on: List of operation IDs that must complete first (for sequential operations)

Operation types:
- semantic_search: Find code by meaning (natural language query)
- symbol_lookup: Find exact symbol definition (function/class name)
- graph_traverse: Follow code dependencies from a symbol
- expand_context: Get code surrounding a result

User question: {query}

Initial context (top 3 results from initial search):
{initial_context}

Generate a program to comprehensively answer this question.
Respond in JSON:
{{
  "reasoning": "Why these operations will find the answer",
  "operations": [
    {{"id": "op1", "type": "...", "query": "...", "parameters": {{}}, "depends_on": []}},
    ...
  ]
}}

Rules:
- Generate 2-5 operations
- Make independent operations parallel (no depends_on)
- Use depends_on when one operation needs another's output
- For "how does X work" questions, combine semantic_search + graph_traverse
- For "find all usages of X" questions, use symbol_lookup + graph_traverse
- For "trace flow" questions, chain graph_traverse operations'''

    QUALITY_EVALUATION_PROMPT = '''Evaluate if these retrieval results answer the user's question.

User question: {query}

Retrieved code (top 5):
{results_summary}

Rate relevance 0-1 and decide:
- Continue: Results are good, no more retrieval needed
- Refine: Results partially relevant, suggest refinement
- Retry: Results are poor, suggest different approach

Respond in JSON:
{{
  "relevance_score": 0.0-1.0,
  "decision": "continue|refine|retry",
  "reasoning": "Why this decision",
  "refinement_suggestion": "If refine/retry, what to try next"
}}'''

    def __init__(
        self,
        config: "Config",
        base_retriever: "HybridRetriever",
        crag_retriever: "CRAGRetriever | None" = None,
        graph_builder: "CodeGraphBuilder | None" = None,
        api_key: str | None = None,
    ) -> None:
        self.config = config
        self.base_retriever = base_retriever
        self.crag_retriever = crag_retriever
        self.graph_builder = graph_builder
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        self._client = None

        # Execution parameters
        self.max_refinement_iterations = 2
        self.quality_threshold = 0.6
        self.parallel_execution = True

    def _get_client(self):
        """Lazily initialize Anthropic client."""
        if self._client is None and self.api_key:
            try:
                from anthropic import Anthropic
                self._client = Anthropic(api_key=self.api_key)
            except ImportError:
                logger.warning("anthropic package not installed")
                return None
        return self._client

    async def execute(
        self,
        query: str,
        initial_result: "RetrievalResult | None" = None,
        limit: int = 20,
    ) -> RLMExecutionResult:
        """
        Execute True RLM retrieval.

        1. Generate retrieval program from LLM
        2. Execute operations (parallel where possible)
        3. Evaluate quality
        4. Refine if needed
        5. Return aggregated results
        """
        execution_trace: list[str] = []
        refinement_count = 0

        # Step 1: Get initial results if not provided
        if initial_result is None:
            initial_result = await self.base_retriever.retrieve(query, limit=10)
            execution_trace.append("Initial retrieval completed")

        # Step 2: Generate retrieval program
        program = await self._generate_program(query, initial_result)
        execution_trace.append(f"Generated program with {len(program.operations)} operations")

        # Step 3: Execute program
        all_results = await self._execute_program(program)
        execution_trace.append(f"Executed {sum(1 for op in program.operations if op.completed)} operations")

        # Step 4: Quality evaluation and refinement loop
        while refinement_count < self.max_refinement_iterations:
            quality = await self._evaluate_quality(query, all_results)
            execution_trace.append(f"Quality evaluation: {quality['relevance_score']:.2f}")

            if quality["decision"] == "continue" or quality["relevance_score"] >= self.quality_threshold:
                break

            # Refine: Generate new operations based on feedback
            refinement_count += 1
            execution_trace.append(f"Refinement iteration {refinement_count}")

            refinement_ops = await self._generate_refinement(
                query, all_results, quality["refinement_suggestion"]
            )

            if refinement_ops:
                program.operations.extend(refinement_ops)
                additional_results = await self._execute_operations(refinement_ops)
                all_results = self._merge_results(all_results, additional_results)
                execution_trace.append(f"Refinement added {len(additional_results)} chunks")

        # Step 5: Aggregate and return
        final_result = self._aggregate_results(all_results, limit)

        # Compute final entropy
        from icd.retrieval.entropy import EntropyCalculator
        entropy_calc = EntropyCalculator()
        final_entropy = entropy_calc.compute_entropy(final_result.scores)

        return RLMExecutionResult(
            chunks=final_result.chunks,
            scores=final_result.scores,
            operations_executed=sum(1 for op in program.operations if op.completed),
            refinement_iterations=refinement_count,
            final_entropy=final_entropy,
            execution_trace=execution_trace,
            metadata={
                "program_reasoning": program.reasoning,
                "total_operations": len(program.operations),
            },
        )

    async def _generate_program(
        self,
        query: str,
        initial_result: "RetrievalResult",
    ) -> RLMProgram:
        """Generate retrieval program using LLM or heuristics."""
        client = self._get_client()

        if client:
            try:
                return await self._llm_generate_program(query, initial_result, client)
            except Exception as e:
                logger.warning(f"LLM program generation failed: {e}")

        # Fallback to heuristic program generation
        return self._heuristic_generate_program(query, initial_result)

    async def _llm_generate_program(
        self,
        query: str,
        initial_result: "RetrievalResult",
        client,
    ) -> RLMProgram:
        """Use LLM to generate retrieval program."""
        import asyncio
        import re

        # Format initial context
        context_parts = []
        for i, chunk in enumerate(initial_result.chunks[:3]):
            context_parts.append(
                f"{i+1}. {chunk.file_path}:{chunk.start_line} ({chunk.symbol_name or 'unknown'})\n"
                f"   {chunk.content[:200]}..."
            )
        initial_context = "\n".join(context_parts) if context_parts else "No initial results"

        prompt = self.PROGRAM_GENERATION_PROMPT.format(
            query=query,
            initial_context=initial_context,
        )

        def call_api():
            response = client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}],
            )
            return response.content[0].text

        loop = asyncio.get_event_loop()
        response_text = await loop.run_in_executor(None, call_api)

        # Parse JSON response
        json_match = re.search(r'\{[\s\S]*\}', response_text)
        if not json_match:
            raise ValueError("No JSON found in LLM response")

        data = json.loads(json_match.group())

        operations = []
        for op_data in data.get("operations", []):
            try:
                op_type = OperationType(op_data.get("type", "semantic_search"))
            except ValueError:
                op_type = OperationType.SEMANTIC_SEARCH

            operations.append(RetrievalOperation(
                op_id=op_data.get("id", f"op{len(operations)}"),
                op_type=op_type,
                query=op_data.get("query", query),
                parameters=op_data.get("parameters", {}),
                depends_on=op_data.get("depends_on", []),
            ))

        logger.info(
            "LLM generated retrieval program",
            num_operations=len(operations),
            reasoning=data.get("reasoning", "")[:100],
        )

        return RLMProgram(
            original_query=query,
            operations=operations,
            reasoning=data.get("reasoning", "LLM-generated program"),
        )

    def _heuristic_generate_program(
        self,
        query: str,
        initial_result: "RetrievalResult",
    ) -> RLMProgram:
        """Generate retrieval program using heuristics."""
        import re

        operations: list[RetrievalOperation] = []
        query_lower = query.lower()

        # Extract potential symbols
        symbols = re.findall(r'\b([A-Z][a-zA-Z0-9]+|[a-z]+_[a-z_]+)\b', query)
        symbols = [s for s in symbols if s not in {"How", "What", "Where", "When", "Why"}]

        # Base semantic search
        operations.append(RetrievalOperation(
            op_id="op1",
            op_type=OperationType.SEMANTIC_SEARCH,
            query=query,
        ))

        # Symbol lookups for identified symbols
        for i, symbol in enumerate(symbols[:2]):
            operations.append(RetrievalOperation(
                op_id=f"op_symbol_{i}",
                op_type=OperationType.SYMBOL_LOOKUP,
                query=symbol,
            ))

        # Graph traversal if we have symbols and a graph
        if symbols and self.graph_builder:
            operations.append(RetrievalOperation(
                op_id="op_graph",
                op_type=OperationType.GRAPH_TRAVERSE,
                query=symbols[0],
                parameters={"direction": "both", "max_hops": 2},
                depends_on=["op_symbol_0"] if symbols else [],
            ))

        # Context expansion for how/trace questions
        if any(q in query_lower for q in ["how does", "trace", "flow"]):
            operations.append(RetrievalOperation(
                op_id="op_expand",
                op_type=OperationType.EXPAND_CONTEXT,
                query=query,
                depends_on=["op1"],
            ))

        return RLMProgram(
            original_query=query,
            operations=operations,
            reasoning="Heuristic program based on query analysis",
        )

    async def _execute_program(self, program: RLMProgram) -> list["Chunk"]:
        """Execute a retrieval program, respecting dependencies."""
        completed: dict[str, RetrievalOperation] = {}
        all_results: list["Chunk"] = []

        while True:
            # Find operations that can be executed (deps satisfied)
            ready_ops = [
                op for op in program.operations
                if not op.completed and all(dep in completed for dep in op.depends_on)
            ]

            if not ready_ops:
                break

            # Execute ready operations in parallel
            if self.parallel_execution and len(ready_ops) > 1:
                tasks = [self._execute_operation(op, completed) for op in ready_ops]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                for op, result in zip(ready_ops, results):
                    if isinstance(result, Exception):
                        logger.warning(f"Operation {op.op_id} failed: {result}")
                        op.completed = True
                    else:
                        op.results = result
                        op.completed = True
                        completed[op.op_id] = op
                        all_results.extend(result)
            else:
                for op in ready_ops:
                    try:
                        result = await self._execute_operation(op, completed)
                        op.results = result
                        op.completed = True
                        completed[op.op_id] = op
                        all_results.extend(result)
                    except Exception as e:
                        logger.warning(f"Operation {op.op_id} failed: {e}")
                        op.completed = True

        return all_results

    async def _execute_operation(
        self,
        op: RetrievalOperation,
        completed: dict[str, RetrievalOperation],
    ) -> list["Chunk"]:
        """Execute a single retrieval operation."""
        logger.debug(f"Executing operation {op.op_id}: {op.op_type.value}")

        if op.op_type == OperationType.SEMANTIC_SEARCH:
            result = await self.base_retriever.retrieve(op.query, limit=20)
            return result.chunks

        elif op.op_type == OperationType.SYMBOL_LOOKUP:
            # Search for exact symbol
            result = await self.base_retriever.retrieve(
                f"definition of {op.query}",
                limit=10,
            )
            # Filter to chunks with matching symbol name
            return [c for c in result.chunks if c.symbol_name and op.query in c.symbol_name]

        elif op.op_type == OperationType.GRAPH_TRAVERSE:
            if not self.graph_builder:
                return []

            # Get seed chunks from dependencies
            seed_chunks = []
            for dep_id in op.depends_on:
                if dep_id in completed:
                    seed_chunks.extend(completed[dep_id].results)

            if not seed_chunks:
                # Try to find the symbol first
                result = await self.base_retriever.retrieve(
                    f"definition of {op.query}",
                    limit=5,
                )
                seed_chunks = result.chunks

            # Traverse graph from seeds
            from icd.graph.traversal import GraphRetriever
            graph_retriever = GraphRetriever(
                self.config,
                self.graph_builder,
                self.base_retriever,
            )

            direction = op.parameters.get("direction", "both")
            expanded = []
            for chunk in seed_chunks[:3]:
                paths = graph_retriever.get_dependency_chain(
                    chunk,
                    direction=direction,
                    max_depth=op.parameters.get("max_hops", 2),
                )
                # Get chunks for nodes in paths
                for path in paths[:5]:
                    for node_id in path:
                        node = self.graph_builder.get_nodes().get(node_id)
                        if node and node.chunk_id:
                            chunks = await self.base_retriever.retrieve_by_ids([node.chunk_id])
                            expanded.extend(chunks)

            return expanded[:20]

        elif op.op_type == OperationType.EXPAND_CONTEXT:
            # Expand context around existing results
            seed_chunks = []
            for dep_id in op.depends_on:
                if dep_id in completed:
                    seed_chunks.extend(completed[dep_id].results)

            if not seed_chunks:
                return []

            chunk_ids = [c.chunk_id for c in seed_chunks[:5]]
            expanded = await self.base_retriever.expand_context(
                chunk_ids,
                max_additional=10,
            )
            return expanded

        else:
            logger.warning(f"Unknown operation type: {op.op_type}")
            return []

    async def _execute_operations(
        self,
        operations: list[RetrievalOperation],
    ) -> list["Chunk"]:
        """Execute a list of operations and return all results."""
        all_results = []

        for op in operations:
            try:
                results = await self._execute_operation(op, {})
                op.results = results
                op.completed = True
                all_results.extend(results)
            except Exception as e:
                logger.warning(f"Operation {op.op_id} failed: {e}")
                op.completed = True

        return all_results

    async def _evaluate_quality(
        self,
        query: str,
        results: list["Chunk"],
    ) -> dict[str, Any]:
        """Evaluate quality of results using LLM or heuristics."""
        client = self._get_client()

        if client and results:
            try:
                return await self._llm_evaluate_quality(query, results, client)
            except Exception as e:
                logger.warning(f"LLM quality evaluation failed: {e}")

        # Fallback to heuristic evaluation
        return self._heuristic_evaluate_quality(query, results)

    async def _llm_evaluate_quality(
        self,
        query: str,
        results: list["Chunk"],
        client,
    ) -> dict[str, Any]:
        """Use LLM to evaluate result quality."""
        import asyncio
        import re

        # Format results summary
        results_parts = []
        for i, chunk in enumerate(results[:5]):
            results_parts.append(
                f"{i+1}. {chunk.file_path}:{chunk.start_line} ({chunk.symbol_name or 'code block'})\n"
                f"   {chunk.content[:300]}..."
            )
        results_summary = "\n".join(results_parts)

        prompt = self.QUALITY_EVALUATION_PROMPT.format(
            query=query,
            results_summary=results_summary,
        )

        def call_api():
            response = client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=500,
                messages=[{"role": "user", "content": prompt}],
            )
            return response.content[0].text

        loop = asyncio.get_event_loop()
        response_text = await loop.run_in_executor(None, call_api)

        json_match = re.search(r'\{[\s\S]*\}', response_text)
        if json_match:
            return json.loads(json_match.group())

        return {"relevance_score": 0.5, "decision": "continue", "reasoning": "Parse error"}

    def _heuristic_evaluate_quality(
        self,
        query: str,
        results: list["Chunk"],
    ) -> dict[str, Any]:
        """Heuristic quality evaluation."""
        if not results:
            return {
                "relevance_score": 0.0,
                "decision": "retry",
                "reasoning": "No results",
                "refinement_suggestion": "Try broader search",
            }

        # Simple relevance scoring based on query term overlap
        query_terms = set(query.lower().split())
        scores = []

        for chunk in results[:10]:
            content_terms = set(chunk.content.lower().split())
            overlap = len(query_terms & content_terms) / len(query_terms) if query_terms else 0
            scores.append(overlap)

        avg_score = sum(scores) / len(scores) if scores else 0

        if avg_score >= 0.6:
            return {
                "relevance_score": avg_score,
                "decision": "continue",
                "reasoning": "Good term overlap",
            }
        elif avg_score >= 0.3:
            return {
                "relevance_score": avg_score,
                "decision": "refine",
                "reasoning": "Partial relevance",
                "refinement_suggestion": "Expand search to related symbols",
            }
        else:
            return {
                "relevance_score": avg_score,
                "decision": "retry",
                "reasoning": "Poor relevance",
                "refinement_suggestion": "Try different query terms",
            }

    async def _generate_refinement(
        self,
        query: str,
        current_results: list["Chunk"],
        suggestion: str,
    ) -> list[RetrievalOperation]:
        """Generate refinement operations based on suggestion."""
        operations = []

        # Heuristic refinement based on suggestion
        if "related symbols" in suggestion.lower():
            # Look for symbols in current results and search for related
            symbols = set()
            for chunk in current_results[:5]:
                if chunk.symbol_name:
                    symbols.add(chunk.symbol_name)

            for i, symbol in enumerate(list(symbols)[:2]):
                operations.append(RetrievalOperation(
                    op_id=f"refine_related_{i}",
                    op_type=OperationType.SEMANTIC_SEARCH,
                    query=f"related to {symbol}",
                ))

        elif "broader" in suggestion.lower():
            # Broader search with simplified query
            simplified = " ".join(query.split()[:5])
            operations.append(RetrievalOperation(
                op_id="refine_broad",
                op_type=OperationType.SEMANTIC_SEARCH,
                query=simplified,
            ))

        else:
            # Default: try graph traversal if available
            if self.graph_builder and current_results:
                operations.append(RetrievalOperation(
                    op_id="refine_graph",
                    op_type=OperationType.GRAPH_TRAVERSE,
                    query=current_results[0].symbol_name or query.split()[0],
                    parameters={"direction": "both", "max_hops": 3},
                ))

        return operations

    def _merge_results(
        self,
        existing: list["Chunk"],
        new: list["Chunk"],
    ) -> list["Chunk"]:
        """Merge results, deduplicating by chunk_id."""
        seen_ids = {c.chunk_id for c in existing}
        merged = list(existing)

        for chunk in new:
            if chunk.chunk_id not in seen_ids:
                seen_ids.add(chunk.chunk_id)
                merged.append(chunk)

        return merged

    def _aggregate_results(
        self,
        chunks: list["Chunk"],
        limit: int,
    ) -> "RetrievalResult":
        """Aggregate chunks into final result."""
        from icd.retrieval.hybrid import RetrievalResult

        # Deduplicate
        seen_ids: set[str] = set()
        unique_chunks: list["Chunk"] = []

        for chunk in chunks:
            if chunk.chunk_id not in seen_ids:
                seen_ids.add(chunk.chunk_id)
                unique_chunks.append(chunk)

        # Simple scoring based on position (earlier = better)
        scores = [1.0 / (i + 1) for i in range(len(unique_chunks))]

        # Take top limit
        final_chunks = unique_chunks[:limit]
        final_scores = scores[:limit]

        # Compute entropy
        from icd.retrieval.entropy import EntropyCalculator
        entropy_calc = EntropyCalculator()
        entropy = entropy_calc.compute_entropy(final_scores)

        return RetrievalResult(
            chunks=final_chunks,
            scores=final_scores,
            entropy=entropy,
            query="",  # Will be set by caller
            metadata={"aggregated_from": len(chunks)},
        )


async def run_true_rlm(
    config: "Config",
    base_retriever: "HybridRetriever",
    query: str,
    crag_retriever: "CRAGRetriever | None" = None,
    graph_builder: "CodeGraphBuilder | None" = None,
    limit: int = 20,
) -> RLMExecutionResult:
    """
    Convenience function to run True RLM retrieval.

    Args:
        config: ICD configuration.
        base_retriever: Base hybrid retriever.
        query: User query.
        crag_retriever: Optional CRAG retriever.
        graph_builder: Optional code graph.
        limit: Maximum results.

    Returns:
        RLM execution result.
    """
    orchestrator = TrueRLMOrchestrator(
        config=config,
        base_retriever=base_retriever,
        crag_retriever=crag_retriever,
        graph_builder=graph_builder,
    )

    return await orchestrator.execute(query, limit=limit)
